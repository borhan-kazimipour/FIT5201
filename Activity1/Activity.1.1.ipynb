{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Activity 1. K-Nearest Neighbour Classifier\n",
    "### Background\n",
    "In this activity, we learn how <a href=\"https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/\">K-Nearest Neighbors (KNN)</a> classifier works. KNN is a simple non-parametric model (ironically non-parametric here means infinite number of parameters), which is an example of <a href=\"https://en.wikipedia.org/wiki/Instance-based_learning\">instance-based</a> supervised learning. We will use KNN as a vehicle to practice some of the basic concepts of machine learning. KNN is a <a href=\"https://en.wikipedia.org/wiki/Lazy_learning\">lazy learner</a> that stores all training data points and their labels in memory, and predict the class label for a new data point based on its similarity to the training data (in fact the stored training data points can be considered as parameters).\n",
    "\n",
    "Consider a training dataset containing (x,t) pairs where $x$ is the input and $t$ is the target class label. Suppose we are given a similarity measure $sim(x_1,x_2)$ which gives the similarity score when fed with two data points. Given a test data point x, the K-nearest neighbour classifier works as follows:\n",
    "<ul>\n",
    "\t<li>Select the top K most similar data points to x from the training set</li>\n",
    "\t<li>Look at the label of the K-nearest neighbours and select the label which has the majority vote.</li>\n",
    "</ul>\n",
    "If the classes are equally common among the neighbours (e.g., two positive and two negative neighbours in binary classification when K=4), the test datapoint is randomly assigned to one of the classes. For example, Figure <strong>A.1</strong> (below) illustrates such situation where the test datapoint (shown by <span style=\"color: #00ff00;\">green</span>) has exactly two neighbours from each class (marked by <span style=\"color: #ff0000;\">red</span> and <span style=\"color: #3366ff;\">blue</span>).\n",
    "\n",
    "<a href=\"http://www.saedsayad.com/k_nearest_neighbors.htm\" rel=\"attachment wp-att-92100\"><img class=\"wp-image-92100 size-full\" src=\"https://www.alexandriarepository.org/wp-content/uploads/20160413152921/A.1.png\" alt=\"Figure A.1: KNN for Classification. The green dot indicates a sample with an unknown class label, while red and blue samples are training observation from default and non-default classes, respectively. Source: http://www.saedsayad.com/k_nearest_neighbors.htm\" width=\"497\" height=\"274\" /></a> \n",
    "\n",
    "> Figure A.1: KNN for Classification. The green dot indicates a sample with an unknown class label, while red and blue samples are training observation from default and non-default classes, respectively. Source: http://www.saedsayad.com/k_nearest_neighbors.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Materials\n",
    "This short <a href=\"https://www.youtube.com/watch?v=UqYde-LULfs\">YouTube video</a> explains KNN and related concepts in a very simple language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for Activity 1\n",
    "<ol>\n",
    "\t<li>Load the iris dataset and divide it to separate training and testing sets,</li>\n",
    "    <li>Define a function that calculates the majority vote,</li>\n",
    "    <li>Define KNN function that takes training labeled samples, testing samples, $K$ and a distance metric and predicts the class labels for the testing samples,</li>\n",
    "\t<li>Apply KNN where for some values of $K$ and report training and testing error</li>\n",
    "\t<li>Plot training and testing error versus $1/K$ where $K \\in \\{1,\\cdots,100\\}$</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the Above Steps\n",
    "Here, we implement a basic KNN classifier. Note that in Assignment 1, you will be asked to expand this implementation and build a KNN regressor. In this task, we use a simple, yet very popular, dataset to investigate the performance of our KNN. \n",
    "\n",
    "### Load and Explor Data\n",
    "Let us start with loading the libraries and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "install.packages(\"reshape2\")\n",
    "install.packages(\"ggplot2\")\n",
    "install.packages(\"corrplot\")\n",
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "library(corrplot)\n",
    "# Load data: it's built in to R, however, you can also get it online\n",
    "# iris <- read.csv(url(\"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"), header = FALSE)\n",
    "library(datasets)\n",
    "data(iris)\n",
    "# take a look at the data\n",
    "head(iris)\n",
    "# Shown are 4 measurements (petal & sepal width & length) for 3 species of iris flowers, where sepal is: \n",
    "# \"One of the usually separate, green parts that surround and protect the flower bud\" (or petals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim(iris) # 150 x 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "install.packages(\"Cairo\")\n",
    "#a few visualizations wont hurt!\n",
    "## the followin plot illustrates petal measurments:\n",
    "ggplot(data=iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) + \n",
    "    geom_point() + geom_rug()+ theme_minimal() + ggtitle(\"Petal Measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## and this one shows the correlation between the features (input variables)\n",
    "#corrplot.mixed(cor(iris[,-5]), lower=\"ellipse\", upper=\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "set.seed(1234)\n",
    "# permute iris, shuffle or mix them up\n",
    "iris <- iris[sample(1:nrow(iris),nrow(iris)),]\n",
    "# create  training and testing subsets:\n",
    "train.index = 1:100\n",
    "train.data <- iris[train.index, -5] # grab the first 100 records, leave out the species (last column)\n",
    "train.label <- iris[train.index, 5]\n",
    "test.data <- iris[-train.index, -5] # grab the last 50 records, leave out the species (last column)\n",
    "test.label <- iris[-train.index, 5]\n",
    "\n",
    "dim(train.data) # 100 records\n",
    "dim(test.data) # 50 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(iris) # the shuffled records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(train.data) # the first 100 records without the Species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an auxiliary function that calculates the majority votes (or mode!)\n",
    "majority <- function(x) {\n",
    "   uniqx <- unique(x)\n",
    "   uniqx[which.max(tabulate(match(x, uniqx)))]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KNN function (distance should be one of euclidean, maximum, manhattan, canberra, binary or minkowski)\n",
    "knn <- function(train.data, train.label, test.data, K=3, distance = 'euclidean'){\n",
    "    ## count number of train samples\n",
    "    train.len <- nrow(train.data)\n",
    "    \n",
    "    ## count number of test samples\n",
    "    test.len <- nrow(test.data)\n",
    "    \n",
    "    ## calculate distances between samples\n",
    "    dist <- as.matrix(dist(rbind(test.data, train.data), method= distance))[1:test.len, (test.len+1):(test.len+train.len)]\n",
    "    \n",
    "    ## for each test sample...\n",
    "    for (i in 1:test.len){\n",
    "        ### ...find its K nearest neighbours from training sampels...\n",
    "        nn <- as.data.frame(sort(dist[i,], index.return = TRUE))[1:K,2]\n",
    "        \n",
    "        ###... and calculate the predicted labels according to the majority vote\n",
    "        test.label[i]<- (majority(train.label[nn]))\n",
    "    }\n",
    "    \n",
    "    ## return the class labels as output\n",
    "    return (test.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let see what is the prediciton of our knn for test samples when K=4\n",
    "knn(train.data, train.label, test.data, K=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and a confusion matrix for K = 5\n",
    "prop.table(table(knn(train.data, train.label, test.data, K=5), test.label))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the train and test missclassification rates for K in 1:100 \n",
    "# THIS MAY TAKE A FEW MINUTES TO COMPLETE!\n",
    "miss <- data.frame('K'=1:100, 'train'=rep(0,100), 'test'=rep(0,100))\n",
    "for (k in 1:100){\n",
    "    miss[k,'train'] <- sum(knn(train.data, train.label, train.data, K=k) != train.label)/nrow(train.data)*100\n",
    "    miss[k,'test'] <-  sum(knn(train.data, train.label, test.data, K=k)  != test.label)/nrow(test.data)*100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot misclassification percentage for train and test data sets\n",
    "miss.m <- melt(miss, id='K') # reshape for visualization\n",
    "names(miss.m) <- c('K', 'type', 'error')\n",
    "ggplot(data=miss.m, aes(x=log(1/K), y=error, color=type)) + geom_line() +\n",
    "       scale_color_discrete(guide = guide_legend(title = NULL)) + theme_minimal() +\n",
    "       ggtitle(\"Missclassification Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Discussions</h1>\n",
    "<ol>\n",
    "\t<li>As $K$ increases, does the complexity of the KNN classifier increase or decrease?</li>\n",
    "\t<li>What is the relationship between $1/K$ and the training error?</li>\n",
    "\t<li>What is the relationship between $1/K$ and the testing error?</li>\n",
    "\t<li>How do you explain the difference between training and testing error trends as the complexity of the KNN classifier increases?</li>\n",
    "    <li>Can you tell the areas where the model overfits and underfits? What is the best value for $K$?</li>\n",
    "\t</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
