{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "In Chapter 3, we have learned that Bootstrapping is a powerful statistical tool that helps us to measure the uncertainty in the prediction of a model. In this activity, we implement this technique to assess variations in the prediction of KNN classifier. In **Assignment Task 2.B.**, you will be asked to expand this example and develop a bootstrap procedure for KNN as a regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for Activity 1.2\n",
    "<ol>\n",
    "\t<li>Load the training dataset,</li>\n",
    "    <li>Implement bootstrap sampling,</li>\n",
    "\t<li>Apply <code>bootstrap </code>function <code> with times = 40</code> (the number of samples) <code>and size = 50 </code> (the size of each sample) on <code>knn </code>for $K\\in{1,\\dots,30}$,</li>\n",
    "\t<li>Report the uncertainty in the prediction in percentages.</li>\n",
    "\t<li>Repeat Step 2 with different values of K, times and size.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the Above Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load libraries:\n",
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "\n",
    "# Load data:\n",
    "library(datasets)\n",
    "data(iris)\n",
    "\n",
    "# permute iris:\n",
    "iris <- iris[sample(1:nrow(iris),nrow(iris)),]\n",
    "# create training and testing datasets:\n",
    "train.index = 1:100\n",
    "train.data <- iris[train.index, -5]\n",
    "train.label <- iris[train.index, 5]\n",
    "test.data <- iris[-train.index, -5]\n",
    "test.label <- iris[-train.index, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function that generates sample indixes based on bootstrap technique\n",
    "boot <- function (original.size=100, sample.size=original.size, times=100){\n",
    "    indx <- matrix(nrow=times, ncol=sample.size)\n",
    "    for (t in 1:times){\n",
    "        indx[t, ] <- sample(x=original.size, size=sample.size, replace = TRUE)\n",
    "    }\n",
    "    return(indx)\n",
    "}\n",
    "# just to see if it works!\n",
    "boot(100, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application in KNN Classifer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is the KNN classifer that we implemented in Activity 1.1\n",
    "\n",
    "# define a function that calculates the majority votes (or mode!)\n",
    "majority <- function(x) {\n",
    "   uniqx <- unique(x)\n",
    "   uniqx[which.max(tabulate(match(x, uniqx)))]\n",
    "}\n",
    "\n",
    "# KNN function (distance should be one of euclidean, maximum, manhattan, canberra, binary or minkowski)\n",
    "knn <- function(train.data, train.label, test.data, K=3, distance = 'euclidean'){\n",
    "    train.len <- nrow(train.data)\n",
    "    test.len <- nrow(test.data)\n",
    "    dist <- as.matrix(dist(rbind(test.data, train.data), method= distance))[1:test.len, (test.len+1):(test.len+train.len)]\n",
    "    for (i in 1:test.len){\n",
    "        nn <- as.data.frame(sort(dist[i,], index.return = TRUE))[1:K,2]\n",
    "        test.label[i]<- (majority(train.label[nn]))\n",
    "    }\n",
    "    return (test.label)\n",
    "}\n",
    "\n",
    "# fix the parameters (50,40,50)\n",
    "K <- 30           # Maximum K for KNN \n",
    "L <- 40           # number of datasets\n",
    "N <- 50          # size of datasets\n",
    "\n",
    "# generate bootstrap indices:\n",
    "boot.indx <- boot(nrow(train.data), N, L)\n",
    "\n",
    "# a dataframe to track the number of missclassified samples in each case\n",
    "miss <- data.frame('K'=1:K, 'L'=1:L, 'test'=rep(0,L*K))\n",
    "\n",
    "# THIS MAY TAKE A FEW MINUTES TO COMPLETE\n",
    "## for every k values:\n",
    "for (k in 1: K){\n",
    "    \n",
    "    ### for every dataset sizes:\n",
    "    for (l in 1:L){\n",
    "        \n",
    "        #### calculate iteration index i\n",
    "        i <- (k-1)*L+l\n",
    "        \n",
    "        #### save sample indices that were selected by bootstrap\n",
    "        indx <- boot.indx[l,]\n",
    "        \n",
    "        #### save the value of k and l\n",
    "        miss[i,'K'] <- k\n",
    "        miss[i,'L'] <- l\n",
    "        \n",
    "        #### calculate and record the train and test missclassification rates\n",
    "        miss[i,'test'] <-  sum(knn(train.data[indx, ], train.label[indx], test.data, K=k)  != test.label)/nrow(test.data)*100\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visalization\n",
    "Let see the output. In the following Jitter Plot, each point corrspends to the error (measured on the fixed test set) of the KNN classifier trained by a random training sample generated during our bootstrap sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot misclassification percentage for train and test data sets\n",
    "miss.m <- melt(miss, id=c('K', 'L')) # reshape for visualization\n",
    "names(miss.m) <- c('K', 'L', 'type', 'miss')\n",
    "ggplot(data=miss.m, aes(x=K, miss, color=type)) + geom_jitter(alpha=0.5)  + \n",
    "    scale_fill_discrete(guide = guide_legend(title = NULL)) + \n",
    "    ggtitle('Missclassifcation vs. K (Jitter Plot)') + theme_minimal() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we produce the box plot of the error numbers (each of which corresponding to a sample in our bootstrap sampling) we have obtained for our test set for each value of K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ggplot(data=miss.m[miss.m$type=='test',], aes(factor(K), miss,fill=type)) + geom_boxplot(outlier.shape = NA)  + \n",
    "    scale_color_discrete(guide = guide_legend(title = NULL)) + \n",
    "    ggtitle('Missclassifcation vs. K (Box Plot)') + theme_minimal()\n",
    "# ignore the warnings (because of ignoring outliers)\n",
    "options(warn=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussions\n",
    "<ol>\n",
    "    <li>As $K$ increases, how does the test error and its uncertainty behave (represented by the boxplots)? </li>\n",
    "    <li>Rerun the above experiments for other values of `times` and `size`, then explain how increasing the number of samples and the size of each sample affect the error uncertainty.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
