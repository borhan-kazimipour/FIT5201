{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 6.2 Linear Regression in Spark\n",
    "In this activity we implement a basic linear regression using Map-Reduce and run it on Spark. From Chapter 6.2 recall that the calculation of the gradient is the computation intensive part of batch gradient descent for linear regression models. Fortunately, we can paralelalise this part of the algorithm to achinve a better performance (in terms of computation time).\n",
    "\n",
    "**Note:** This activity is developed based on:\n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/logistic_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # numpy provides lots of math functions\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions\n",
    "Let's start with defining some auxiliray functions to make our code clean and clear. The first function is the `add` function that we will use it in the `reduce` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add function!\n",
    "def add(x,y): return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the `readPointBatch` fucntion  that will be used as a partition mapper. Indeed, `readPointBatch` maps the text partitions, that were produced by `textFile` function,  into partitions contatining a chunk of the data matrix (i.e., batches of float numbers). To be able to implement our algorithm using Spark features, the data has to be in certain format. For example, in this activity we assume that our data is space ' ' separated and stored in $<label> <x_1> <x_2> \\dots <x_D>$ format ($D$ is number of dimensions and $label$ is numeric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following function converts the content of the file to an specific convention (will be discussed)\n",
    "# it also partitiones data to some batches\n",
    "def readPointBatch(iterator,D):\n",
    "    strs = list(iterator)\n",
    "    matrix = np.zeros((len(strs), D + 1))\n",
    "    for i, s in enumerate(strs):\n",
    "        matrix[i] = np.fromstring(s.replace(',', ' '), dtype=np.float32, sep=' ')\n",
    "    return [matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement a simple function that will be helpfull in counting the number of datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the number of points in a matrix\n",
    "def num_points(matrix):\n",
    "    return np.size(matrix,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define the  `gradient` and `error` functions. They get data as `matrix` and weight vector `w`, and then compute the gradeint and the error according to the given weight vector. We will use these functions in the body of our linear regression code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computes linear regression gradient for a matrix of data points\n",
    "def gradient(matrix, w):\n",
    "    T = matrix[:, 0]    # point labels (first column of input file)\n",
    "    X = matrix[:, 1:]   # point coordinates\n",
    "    X = np.concatenate((X, np.ones((np.size(X,0),1))), axis=1) # add a column of all 1 (for intercept)\n",
    "    # For each point (x, y), compute gradient function, then sum these up\n",
    "    return (-(T-X.dot(w)) * X.T).sum(1)\n",
    "\n",
    "# Computes the training error of the given data (matrix)\n",
    "def error(matrix, w):\n",
    "    T = matrix[:, 0]    # point labels (first column of input file)\n",
    "    X = matrix[:, 1:]   # point coordinates\n",
    "    X = np.concatenate((X, np.ones((np.size(X,0),1))), axis=1) # add a column of all 1 (for intercept)\n",
    "    # For each point (x, y), compute gradient function, then sum these up\n",
    "    return ((T-X.dot(w))**2).sum(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Here, we create an Spark Context object, then read `LR_small.csv` file and slipt it into some partitions using `textFile` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.2306114085435,1.75950063212509,0.294971276590044,0.0227169929355219,-0.776138189567377,-0.150155898031874,0.403247739988761,1.45590127802096,2.96063439859703,0.659921119676468,0.503935021819967\r",
      "\r\n",
      "21.0794747912023,0.67162075775209,0.318028893848439,0.622494365889553,0.620427476079081,-0.517798639281123,1.36043562440755,-0.338959993266462,1.17624173097219,0.397550204420852,2.94558490754999\r",
      "\r\n",
      "32.2110092367841,0.665508067881952,-0.338941026863734,2.00638441797567,0.277773873821185,0.471737329436623,0.148166488423676,0.671361913782469,0.500984658985886,0.901695285362934,1.15337845186424\r",
      "\r\n",
      "30.6320519229193,0.0791468620838836,-0.914062848484401,-0.0587639170364419,0.223213119649714,0.964962108814435,1.23868047810971,-2.00224447540798,0.0739783801991804,0.300527612949985,-1.34537964435454\r",
      "\r\n",
      "31.8788824580358,0.550740756579486,0.248873615876031,0.033313832093629,0.559606665724997,-0.582384249448539,0.126167677363352,-0.126508647389784,-1.23116316475166,1.02365966191829,0.346478436733037\r",
      "\r\n",
      "42.0987971155355,0.0567227047663041,-0.736312284391046,0.868231024468119,1.59002149495523,-0.362456446353642,1.05381026510739,0.702300412107003,-1.40684022320326,1.676169355935,0.177637252541911\r",
      "\r\n",
      "16.6145511199745,-1.21543538511763,-0.862940388413092,-0.693026314196571,1.66315211508538,-0.54597047848206,-1.20885450419493,0.108690364352581,1.20890389431148,-2.29207682681525,0.325665051745599\r",
      "\r\n",
      "8.04026890768856,0.113265796089827,-1.05787286175932,0.0360661972691989,-1.85294515009871,-0.450511650994497,-0.237310554205392,-0.251885087051611,-0.696901042439571,-1.14246702776361,1.23940534300231\r",
      "\r\n",
      "36.2406739010511,1.36723504874826,-0.803092811045523,0.271154144184996,-0.171883319453584,-0.976451010784578,-0.662063826984902,0.234703638848023,1.4450524887899,1.18492519577509,-0.883267403866549\r",
      "\r\n",
      "45.5247518911001,1.6675872724652,-0.652354638187269,-1.4127155696086,1.37902345396802,1.09520742804393,0.646937045698563,0.298789361900239,-0.539352893869873,1.06296684614244,-1.37508939793364\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Initiate Spark\n",
    "sc.stop()  # just in case there is a crashed/running context!\n",
    "sc = SparkContext() # initiate an new context\n",
    "# Read and distribute the file \n",
    "points = sc.textFile(\"./LR_small.csv\")\n",
    "# in case you want to take a look at the data (each line contains 1 target and 10 predictor values)\n",
    "!head ./LR_all.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to convert the text strings into matrices using `readPointBatch` function. Note that each partition of the input text should be convereted to a single matrix (that's why we use `mapPartitions`). When the convertion is done, we store the matrices in cashe for future uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = 10   # Number of dimensions.\n",
    "# read batch of points and store in cache\n",
    "points = points.mapPartitions(lambda j: readPointBatch(j,D)).cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "The follwoing is our Linear Regression implementation using Spark. Let's start with the initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initalizations:\n",
    "N = points.map(lambda m: num_points(m)).reduce(add)\n",
    "iterations = 500 # linear regression maximum iterations\n",
    "errors = np.zeros((iterations+1, 1))\n",
    "weights = np.zeros((iterations+1, D + 1))\n",
    "eta = 0.0001 # initial value for learning rate\n",
    "w = 2 * np.random.ranf(size=(D+1)) - 1 # Initialize w to random values [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the main loop. The key line in this loop is `w -= points.map(lambda m: gradient(m, w)).reduce(add) * eta` which updates the weight vector `w` based on the sum of the gradeints (multiplied by the learning rate `eta`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On iteration 1 Error: 14.333726\n",
      "On iteration 11 Error: 12.890932\n",
      "On iteration 21 Error: 11.646359\n",
      "On iteration 31 Error: 10.572582\n",
      "On iteration 41 Error: 9.646236\n",
      "On iteration 51 Error: 8.847354\n",
      "On iteration 61 Error: 8.158821\n",
      "On iteration 71 Error: 7.565912\n",
      "On iteration 81 Error: 7.055906\n",
      "On iteration 91 Error: 6.617776\n",
      "On iteration 101 Error: 6.241927\n",
      "On iteration 111 Error: 5.919986\n",
      "On iteration 121 Error: 5.644637\n",
      "On iteration 131 Error: 5.409479\n",
      "On iteration 141 Error: 5.208916\n",
      "On iteration 151 Error: 5.038066\n",
      "On iteration 161 Error: 4.892673\n",
      "On iteration 171 Error: 4.769046\n",
      "On iteration 181 Error: 4.663990\n",
      "On iteration 191 Error: 4.574748\n",
      "On iteration 201 Error: 4.498955\n",
      "On iteration 211 Error: 4.434582\n",
      "On iteration 221 Error: 4.379899\n",
      "On iteration 231 Error: 4.333432\n",
      "On iteration 241 Error: 4.293927\n",
      "On iteration 251 Error: 4.260322\n",
      "On iteration 261 Error: 4.231715\n",
      "On iteration 271 Error: 4.207346\n",
      "On iteration 281 Error: 4.186568\n",
      "On iteration 291 Error: 4.168837\n",
      "On iteration 301 Error: 4.153693\n",
      "On iteration 311 Error: 4.140745\n",
      "On iteration 321 Error: 4.129664\n",
      "On iteration 331 Error: 4.120172\n",
      "On iteration 341 Error: 4.112032\n",
      "On iteration 351 Error: 4.105046\n",
      "On iteration 361 Error: 4.099043\n",
      "On iteration 371 Error: 4.093879\n",
      "On iteration 381 Error: 4.089434\n",
      "On iteration 391 Error: 4.085603\n",
      "On iteration 401 Error: 4.082297\n",
      "On iteration 411 Error: 4.079443\n",
      "On iteration 421 Error: 4.076976\n",
      "On iteration 431 Error: 4.074841\n",
      "On iteration 441 Error: 4.072992\n",
      "On iteration 451 Error: 4.071389\n",
      "On iteration 461 Error: 4.069998\n",
      "On iteration 471 Error: 4.068790\n",
      "On iteration 481 Error: 4.067739\n",
      "On iteration 491 Error: 4.066825\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression MAIN loop\n",
    "for i in range(iterations):\n",
    "    # keep the record of errors\n",
    "    errors[i, 0] = (points.map(lambda m: error(m, w)).reduce(add))**(0.5)/N\n",
    "    # keep the record of weights\n",
    "    weights[i,:] = w\n",
    "    # display the progress every 10 iterations\n",
    "    if i%10 ==0:\n",
    "        print(\"On iteration %i Error: %f\" % (i + 1,  errors[i,0]))\n",
    "    # calculate the new weights\n",
    "    w -= points.map(lambda m: gradient(m, w)).reduce(add) * eta # map & reduce!\n",
    "    # decrease the learning rate (for convergence)\n",
    "    eta = eta * 0.9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Let's see the final weights and error...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final w: [  1.1868586    2.09777429   0.25298232   2.4092738    2.92885362\n",
      "   2.02913539   2.16060976   0.61416484   1.54842508   2.03867811\n",
      "  28.84178787]\n",
      "Final error: 4.06602861095\n"
     ]
    }
   ],
   "source": [
    "errors[-1, 0] = (points.map(lambda m: error(m, w)).reduce(add))**(0.5)/N\n",
    "weights[-1,:] = w\n",
    "\n",
    "print(\"Final w: \" + str(w))\n",
    "print(\"Final error: \" + str((points.map(lambda m: error(m, w)).reduce(add))**(0.5)/N))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and draw the convergence plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEZCAYAAAB1mUk3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70743c0d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(errors)\n",
    "plt.title('Convergence Plot')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
